# Llama
<h1>Llama from scratch</h1>

This repository is the implementation of the <a href="https://arxiv.org/pdf/2302.13971.pdf" target="_blank">Llama</a> article published by Meta AI in Feb 2023. LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs) ranging from 7B to 65B parameters. Like other LLMs, Llama is transformer-based. As the authors mentioned in their paper, they want to show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. They demonstrate that LLaMA-13B outperforms GPT-3(175B) on most benchmarks and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B.

